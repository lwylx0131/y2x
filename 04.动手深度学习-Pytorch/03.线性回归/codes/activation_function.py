# -*- encoding: utf-8 -*-

import torch
import numpy as np

# ReLU(rectified linear unit)函数提供了一个简单的非线性变换，给定元素x，该函数定义为：
# ReLU(x) = max(x, 0) 只保留正数元素，并将负数元素清零。
x = torch.arange(-2.0, 2.0, 0.1, requires_grad=True)
y = x.relu()
'''
tensor([-2.0000, -1.9000, -1.8000, -1.7000, -1.6000, -1.5000, -1.4000, -1.3000,
        -1.2000, -1.1000, -1.0000, -0.9000, -0.8000, -0.7000, -0.6000, -0.5000,
        -0.4000, -0.3000, -0.2000, -0.1000,  0.0000,  0.1000,  0.2000,  0.3000,
         0.4000,  0.5000,  0.6000,  0.7000,  0.8000,  0.9000,  1.0000,  1.1000,
         1.2000,  1.3000,  1.4000,  1.5000,  1.6000,  1.7000,  1.8000,  1.9000],
       requires_grad=True)
tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
        0.0000, 0.0000, 0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000,
        0.7000, 0.8000, 0.9000, 1.0000, 1.1000, 1.2000, 1.3000, 1.4000, 1.5000,
        1.6000, 1.7000, 1.8000, 1.9000], grad_fn=<ReluBackward0>)
'''
# 当输入负数时，ReLU函数的导数为0，当输入正数时，ReLU函数的导数为1
y.sum().backward()
x.grad
'''
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1.])
'''

# sigmoid函数可以将元素的值变换到0和1之间：sigmoid(x) = 1 / (1 + exp(-x))
# 当输入接近0时，sigmoid函数接近线性变换
y = x.sigmoid()
# 根据链式法则，sigmoid函数的导数：sigmoid'(x) = sigmoid(x)(1 - sigmoid(x))
# 当输入为0时，sigmoid函数的导数达到最大值0.25；当输入越偏离0时，sigmoid函数的导数越接近0
x.grad.zero_()
y.sum().backward()
x.grad

# tanh函数可以将元素的值变换到-1和1之间：tanh(x) = (1 - exp(-2x)) / (1 + exp(-2x))
# 当输入接近0时，tanh函数接近线性变换。虽然函数形状和sigmoid函数很像，但tanh函数在坐标系的原点上对称
y = x.tanh()
# 根据链式法则，tanh函数的导数：tanh'(x) = 1 = tanh**2(x)
# 当输入为0时，tanh函数的导数达到最大值1；当输入越偏离0时，tanh函数的导数越接近0
x.grad.zero_()
y.sum().backward()
x.grad