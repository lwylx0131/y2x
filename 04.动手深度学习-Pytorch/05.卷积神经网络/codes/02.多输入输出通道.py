# -*- encoding: utf-8 -*-

#%matplotlib inline
import torch
import numpy as np
import pandas as pd
import sys
sys.path.append('../..')
import dl_common_pytorch as dl
import torch.nn as nn
print(torch.__version__)
torch.set_default_tensor_type(torch.FloatTensor)

'''
之前讨论的都是输入和输出的二维数组，但真实数据的维度经常更高。例如，彩色图像在高和宽2个维度外还有RGB(红绿蓝)3个颜色通道。
假设彩色图像的高和宽分别是h和w(像素)，那么它可以表示为一个3*h*w的多维数组。我们将大小为3的这一维称为通道(channel)维。
'''
# 通过代码实现含多个输入通道的互相关运算，只需要对每个通道做互相关运算，然后再通过add_n函数来进行累加。
def corr2d_multi_in(X, K):
    # 沿着X和K的第0维（通道维）分别计算再相加
    res = dl.corr2d(X[0, :, :], K[0, :, :])
    for i in range(1, X.shape[0]):
        res += dl.corr2d(X[i, :, :], K[i, :, :])
    return res

# 多输入通道：构造输入数组X、核数组K来验证互相关运算的输出
X = torch.tensor([[[0, 1, 2], [3, 4, 5], [6, 7, 8]], [[1, 2, 3], [4, 5, 6], [7, 8, 9]]])
K = torch.tensor([[[0, 1], [2, 3]], [[1, 2], [3, 4]]])
print(corr2d_multi_in(X, K))
'''
tensor([[ 56.,  72.],
        [104., 120.]])
'''

# 当输入通道有多个时，因为我们对各个通道的结果做了累加，所以不论输入通道数是多少，输出通道数总是为1。
# 设卷积核输入通道数和输出通道数分别为ci和co，高和宽分别为kh和kw。如果希望得到含多个通道的输出，我们可以为每个输出通道分别创建形状为ci*kh*kw的核数组。
# 将它们在输出通道维上连接，卷积核的形状即co*ci*kh*kw。
# 在做互相关运算时，每个输出通道上的结果由卷积核在该输出通道上的核数组与整个输入数组计算而来。
# 实现一个互相关运算函数来计算多个通道的输出
def corr2d_multi_in_out(X, K):
    # 对K的第0维遍历，每次同输入X做互相关计算。所有结果使用stack函数合并在一起
    return torch.stack([corr2d_multi_in(X, k) for k in K])
# 将核数组K同K+1(K中每个元素加1)和K+2连接在一起来构造一个输出通道数位3的卷积核
K = torch.stack([K, K+1, K+2])
K.shape # torch.Size([3, 2, 2, 2])
# 对输入数组X与核数组K做互相关运算。此时的输出含有3个通道。其中第一个通道结果与之前输入数组X与多输入通道、但输出通道核的计算结果一致。
print(corr2d_multi_in_out(X, K))
'''
tensor([[[ 56.,  72.],
         [104., 120.]],

        [[ 76., 100.],
         [148., 172.]],

        [[ 96., 128.],
         [192., 224.]]])
'''

# 卷积窗口形状为1*1(kh=kw=1)的多通道卷积层，通常成为1*1卷积层，并将其中的卷积运算成为1*1卷积。
# 1*1卷积核可在不改变高宽的情况下，调整通道数。1*1卷积核不识别高和宽维度上相邻元素构成的模式，其主要计算发生在通道维上。
# 假设我们将通道维当作特征维，将高和宽维度上的元素当成数据样本，那么1*1卷积层的作用与全连接层等价。

'''
卷积层与全连接层的对比：
二维卷积层经常用于处理图像，与此前的全连接层相比，它主要有两个优势：
一是全连接层把图像展平成一个向量，在输入图像上相邻的元素可能因为展平操作不再相邻，网络难以捕捉局部信息。而卷积层的设计，天然地具有提取局部信息的能力。
二是卷积层的参数量更少。不考虑偏置的情况下，一个形状为(ci,co,h,w)的卷积核的参数量是ci*co*h*w，与输入图像的宽高无关。
假如一个卷积层的输入和输出形状分别是(c1,h1,w1)和(c2,h2,w2)，如果要用全连接层进行连接，参数数量就是c1*c2*h1*w1*h2*w2。
使用卷积层可以以较少的参数数量来处理更大的图像。
'''
# 通过PyTorch中的nn.Conv2d类来实现二维卷积层
# forward函数的参数为一个4维张量，形状为(N,Cin,Hin,Win)，返回值也是一个4维张量，形状为(N,Cout,Hout,Wout)
# 其中N是批量大小，C/H/W分别表示通道数，高度，宽度
X = torch.rand(4, 2, 3, 5)
print(X.shape)

conv2d = nn.Conv2d(in_channels=2, out_channels=3, kernel_size=(3, 5), stride=1, padding=(1, 2))
Y = conv2d(X)
print('Y.shape = ', Y.shape)
print('weight.shape = ', conv2d.weight.shape)
print('bias.shape = ', conv2d.bias.shape)
'''
torch.Size([4, 2, 3, 5])
Y.shape =  torch.Size([4, 3, 3, 5])
weight.shape =  torch.Size([3, 2, 3, 5])
bias.shape =  torch.Size([3])
'''

'''
池化层主要用于缓解卷积层对位置的过度敏感性。同卷积层一样，池化层每次对输入数据的一个固定形状窗口（又称池化窗口）中的元素计算输出，
池化层直接计算池化窗口内元素的最大值或者平均值，该运算也分别叫做最大池化或平均池化。

二维平均池化的工作原理与二维最大池化类似，但将最大运算符替换成平均运算符。池化窗口形状为p*q的池化层称为池化层，其中的池化运算叫p*q作池化。
池化层也可以在输入的高和宽两侧填充并调整窗口的移动步幅来改变输出形状。池化层填充和步幅与卷积层填充和步幅的工作机制一样。
在处理多通道输入数据时，池化层对每个输入通道分别池化，但不会像卷积层那样将各通道的结果按通道相加。这意味着池化层的输出通道数与输入通道数相等。
'''
# 使用PyTorch中的nn.MaxPool2d实现最大池化层。平均池化层使用的是nn.AvgPool2d，使用方法与nn.MaxPool2d相同。
# forward函数的参数为一个4维张量，形状为(N,C,Hin,Win)，返回值也是一个4维张量，形状为(N,C,Hout,Wout)，其中N是批量大小，C,H,W分别表示通道数、高度、宽度。
X = torch.arange(32, dtype=torch.float32).view(1, 2, 4, 4)
pool2d = nn.MaxPool2d(kernel_size=3, padding=1, stride=(2, 1))
Y = pool2d(X)
print(X)
print(Y)
'''
tensor([[[[ 0.,  1.,  2.,  3.],
          [ 4.,  5.,  6.,  7.],
          [ 8.,  9., 10., 11.],
          [12., 13., 14., 15.]],

         [[16., 17., 18., 19.],
          [20., 21., 22., 23.],
          [24., 25., 26., 27.],
          [28., 29., 30., 31.]]]])
tensor([[[[ 5.,  6.,  7.,  7.],
          [13., 14., 15., 15.]],

         [[21., 22., 23., 23.],
          [29., 30., 31., 31.]]]])
'''