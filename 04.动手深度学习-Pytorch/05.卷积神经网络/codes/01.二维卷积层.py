# -*- encoding: utf-8 -*-

#%matplotlib inline
import torch
import numpy as np
import pandas as pd
import sys
sys.path.append('../..')
import dl_common_pytorch as dl
import torch.nn as nn
print(torch.__version__)
torch.set_default_tensor_type(torch.FloatTensor)

'''
卷积神经网络（convolutional neural network）是含有卷积层（convolutional layer）的神经网络。
本章中介绍的卷积神经网络均使用最常见的二维卷积层，它有高和宽两个空间维度，常用来处理图像数据。

二维互相关（cross-correlation）运算的输入是一个二维输入数组和一个二维核（kernel）数组，输出也是一个二维数组，其中核数组通常称为卷积核或过滤器（filter）。
卷积核的尺寸通常小于输入数组，卷积核在输入数组上滑动，在每个位置上，卷积核与该位置处的输入子数组按元素相乘并求和，得到输出数组中相应位置的元素。
'''
X = torch.tensor([[0, 1, 2], [3, 4, 5], [6, 7, 8]])
K = torch.tensor([[0, 1], [2, 3]])
print(dl.corr2d(X, K))

# 二维卷积层将输入和卷积核做互相关运算，并加上一个标量偏差来得到输出。
# 卷积层的模型参数包括了卷积核和标量偏差。在训练模型的时候，通常我们先对卷积核随机初始化，然后不断迭代卷积核和偏差。
# 以下基于corr2d函数实现一个自定义的二维卷积层
class Conv2D(nn.Module):
    def __init__(self, kernel_size):
        super(Conv2D, self).__init__()
        self.weight = nn.Parameter(torch.randn(kernel_size))
        self.bias = nn.Parameter(torch.randn(1))
    def forward(self, x):
        return dl.corr2d(x, self.weight) + self.bias
# 卷积窗口形状为p*q的卷积层称为p*q卷积层，说明卷积核的高和宽分别为p和q

# 卷积层简单应用：检测图像中的物体的边缘，即找到像素变化的位置。
# 首先构造一张6*8的图像（即高和宽分别为6像素和8像素的图像）：它的中间4列为黑（0），其余为白（1）
X = torch.ones((6, 8))
X[:, 2:6] = 0
'''
tensor([[1., 1., 0., 0., 0., 0., 1., 1.],
        [1., 1., 0., 0., 0., 0., 1., 1.],
        [1., 1., 0., 0., 0., 0., 1., 1.],
        [1., 1., 0., 0., 0., 0., 1., 1.],
        [1., 1., 0., 0., 0., 0., 1., 1.],
        [1., 1., 0., 0., 0., 0., 1., 1.]])
'''
# 再构造一个高和宽分别为1和2的卷积核K，当它与输入做互相关运算时，如果横向相邻元素相同，输出为0，否则输出为非0
K = torch.tensor([[1, -1]])
Y = dl.corr2d(X, K)
'''
可以看到从白到黑的边缘和从黑到白的边缘分别检测成了1和-1，其余部分的输出全是0。
可以看出，卷积层可通过重复使用卷积核有效的表征局部空间。
tensor([[0., 1., 0., 0., 0., -1., 0.],
        [0., 1., 0., 0., 0., -1., 0.],
        [0., 1., 0., 0., 0., -1., 0.],
        [0., 1., 0., 0., 0., -1., 0.],
        [0., 1., 0., 0., 0., -1., 0.],
        [0., 1., 0., 0., 0., -1., 0.]])
'''

# 使用物体边缘检测中的输入数据X和输出数据Y来学习我们构造的核数组K。
# 首先构造一个卷积层，其卷积核将被初始化成随机数组。接下来每一次迭代中，使用平方误差来比较Y和卷积层的输出，然后计算梯度来更新权重。
# 构造一个核数组形状是(1, 2)的二维卷积层
net = Conv2D(kernel_size=(1, 2))
step = 20
lr = 0.01
for i in range(step):
    Y_hat = net(X)
    l = ((Y_hat - Y) ** 2).sum()
    l.backward()
    
    # 梯度下降
    net.weight.data -= lr * net.weight.grad
    net.bias.data -= lr * net.bias.grad
    
    # 梯度清零
    net.weight.grad.fill_(0)
    net.bias.grad.fill_(0)
    if(i + 1) % 5 == 0:
        print('step %d, loss %.3f' %(i+1, l.item()))
print('weight: ', net.weight.data)
print('bias: ', net.bias.data)
'''
可以看到，20次迭代后误差已经降到一个比较小的值，学到的卷积核的权重参数与我们之前定义的核数组K[1, -1]比较接近，而偏置参数接近0
step 5, loss 6.490
step 10, loss 1.588
step 15, loss 0.418
step 20, loss 0.114
weight:  tensor([[ 0.9090, -0.9211]])
bias:  tensor([0.0068])
'''

'''
实际上，卷积运算与互相关运算类似。为了得到卷积运算的输出，我们只需要将核数组左右翻转并上下翻转，再与输入数组做互相关运算。
可见，卷积运算和互相关运算虽然类似，但如果它们使用相同的核数组，对于同一个输入，输出往往并不相同。
为何卷积层能够使用互相关运算替代卷积运算。其实，在深度学习中核数组都是学出来的：卷积层无论使用互相关运算或者卷积运算都不影响模型预测时的输出。
'''

'''
特征图与感受野：
二维卷积层输出的二维数组可以看作是输入在空间维度（宽和高）上某一级的表征，也叫特征图（feature map）。
影响元素x的前向计算的所有可能输入区域（可能大于输入的实际尺寸）叫做的感受野（receptive field）。

以图1为例，输入中阴影部分的四个元素是输出中阴影部分元素的感受野。我们将图中形状为2*2的输出记为Y，将Y与另一个形状为2*2的核数组做互相关运算，输出单个元素z。
那么，z在Y上的感受野包括Y的全部四个元素，在输入上的感受野包括其中全部9个元素。
可见，我们可以通过更深的卷积神经网络使特征图中单个元素的感受野变得更加广阔，从而捕捉输入上更大尺寸的特征。
'''

'''
当使用高和宽为3的输入与高和宽为2的卷积核得到高和宽为2的输出。
一般来说，假设输入形状为nh*nw，卷积核窗口形状是kh*kw，那么输出形状将会是：(nh-kh+1) * (nw-kw+1)
所以卷积层的输出形状由输入形状和卷积核窗口形状决定。
填充和步幅这两个卷积层的超参数，可以对给定形状的输入和卷积核改变输出形状。

填充（padding）是指在输入高和宽的两侧填充元素（通常是0元素）。
比如在原输入高和宽的两侧分别添加了值为0的元素，使得输入高和宽从3变成了5，并导致输出高和宽由2增加到4.
一般来说，如果在高的两侧一共填充ph行，在宽的两侧一共填充pw列，那么输出的形状将会是：(nh-kh+ph+1) * (nw-kw+pw+1)
也就是说，输出的高和宽会分别增加ph和pw。
很多情况下，我们会设置ph=kh-1和pw=kw-1来使输入和输出具有相同的高和宽，这样会方便在构造网络时推测每个层的输出形状。
假设这里kh是奇数，则会在高的两侧分别填充ph/2。如果kh是偶数，一种可能是输入的顶端一侧填充ph/2行，而在底端一侧填充ph/2行，在宽的两侧填充同理。
卷积神经网络经常使用奇数高宽的卷积核，如1、3、5、7，所以两端上的填充个数相等。
对任意的二维数组X，设它的第i行第j列的元素为X[i,j]。当两端上的填充个数相等，并使输入和输出具有相同的高和宽时，
我们就知道输出Y[i,j]是由输入以X[i,j]为中心的窗口同卷积核进行互相关计算得到的。
'''
# 创建一个高和宽为3的二维卷积层，然后设输入高和宽两侧的填充数分别为1。给定一个高和宽为8的输入，我们发现输出的高和宽也是8。
# 定义一个函数来计算卷积层。它对输入和输出做相应的升维和降维
def comp_conv2d(py_conv2d, X):
    # (1, 1)代表批量大小和通道数均为1
    X = X.view((1, 1) + X.shape) # ((1, 1) + (8, 8)) = (1, 1, 8, 8) = torch.size([1, 1, 8, 8])
    Y = py_conv2d(X)
    return Y.view(Y.shape[2:]) # 计算Y结果为[1, 1, 8, 8]，排除前面不关心的两维[1, 1]

# 这里是两侧分别填充1行或列，所以在两侧一共填充2行或列
py_conv2d = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding=1)
X = torch.rand(8, 8)
print(comp_conv2d(py_conv2d, X).shape)
# torch.Size([8, 8])

# 当卷积核的高和宽不同时，我们也可以通过设置高和宽上不同的填充数使输出和输入具有相同的高和宽
# 使用高为5、宽为3的卷积核。在高和宽两侧的填充数分别为2和1
py_conv2d = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(5, 3), padding=(2, 1))
print(comp_conv2d(py_conv2d, X).shape)
# torch.Size([8, 8])

'''
卷积窗口从输入数组的最左上方开始，按从左往右，从上往下的顺序，依次在输入数组上滑动。我们将每次滑动的行数和列数称为步幅（stride）。
目前我们的例子中，在高和宽两个方向上步幅均为1，也可以使用更大的步幅。比如在高上步幅为3、在宽上步幅为2的二维互相关运算。
'''
# 令高和宽上的步幅均为2，从而使输入的高和宽减半
py_conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1, stride=2)
print(comp_conv2d(py_conv2d, X).shape)
# torch.Size([4, 4])
py_conv2d = nn.Conv2d(1, 1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))
print(comp_conv2d(py_conv2d, X).shape)
# torch.Size([2, 2])

'''
简单来说：当输入的高和宽两侧的填充数分别为ph和pw时，我们称填充为(ph, pw)。特别地，当ph=pw=p时，填充为p。
当在高和宽上的步幅分别为sh和sw时，我们称步幅为(sh,sw)。特别地，当sh=sw=s时，步幅为s。在默认情况下，填充为0，步幅为1。

填充可以增加输出的高和宽。这常用来使输出与输入具有相同的高和宽。
步幅可以减少输出的高和宽，例如输出的高和宽仅为输入的高和宽的1/n。
'''